# -*- coding: utf-8 -*-
"""AI chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxURcia7Mp-QDNuYWO1FGqBT0s21gy4F

## **環境**
"""

# huggingface transformer のインストール
# - transformers : 主たるモジュール（モデルやトークナイザのダウンロード)
# - datasets : HuggingFaceで、データセットを扱うためのモジュール
! pip install transformers datasets

# 日本語用BERT使用に必要なパッケージをインストール
! pip install fugashi ipadic

!pip install accelerate
from google.colab import userdata
from google.colab import drive

# Googleドライブのマウント
drive.mount('/content/drive')

import numpy as np
import pandas as pd

# Hugging Face (Transformers) 関連のモジュール
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from datasets import Dataset, load_metric

# 前準備 Matplotlib 日本語フォント

!apt-get -y install fonts-ipafont-gothic
!rm /root/.cache/matplotlib/fontlist-v310.json

# ランタイムを再起動

!pip install japanize-matplotlib

import matplotlib.pyplot as plt
import japanize_matplotlib
import seaborn as sns
sns.set(font='IPAexGothic', font_scale=0.9)

# 動作確認
plt.figure(figsize=(5,1))
plt.title('日本語を表示できるかテスト')

"""## **データセット**"""

# WRIMEデータダウンロード
# WRIME dataset
! wget https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv

# pandas.DataFrameとして読み込む
df_wrime = pd.read_table('wrime-ver1.tsv')
df_wrime.head(2)

"""## **前処理**

"""

# Plutchikの8つの基本感情
emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']
emotion_names_jp = ['喜び', '悲しみ', '期待', '驚き', '怒り', '恐れ', '嫌悪', '信頼']  # 日本語版
num_labels = len(emotion_names)

# readers_emotion_intensities 列を生成する
# "Avg. Readers_*" の値をlist化
df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)

# 感情強度が低いサンプルは除外
# (readers_emotion_intensities の max が２以上のサンプルのみを対象とする)
is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 2)
df_wrime_target = df_wrime[is_target]

# train / test に分割する
df_groups = df_wrime_target.groupby('Train/Dev/Test')
df_train = df_groups.get_group('train')
df_test = pd.concat([df_groups.get_group('dev'), df_groups.get_group('test')])
print('train :', len(df_train))
print('test :', len(df_test))

"""## **モデル訓練(BERT)**"""

# 使用するモデルを指定して、Tokenizerを読み込む
checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# 前処理関数: tokenize_function
# 感情強度の正規化（総和=1）も同時に実施する
def tokenize_function(batch):
    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length')
    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]  # 総和=1に正規化
    return tokenized_batch

# Transformers用のデータセット形式に変換
# pandas.DataFrame -> datasets.Dataset
target_columns = ['Sentence', 'readers_emotion_intensities']
train_dataset = Dataset.from_pandas(df_train[target_columns])
test_dataset = Dataset.from_pandas(df_test[target_columns])

# 前処理（tokenize_function） を適用
train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)
test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)

""" 学習済みモデル読み込み"""

# 分類モデルのため AutoModelForSequenceClassification を使用する
# checkpoint と num_labels（クラス数） を指定する. 今回は、いずれも上で定義済み
# - checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'
# - num_labels = 8
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)

"""学習"""

# 評価指標を定義
metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    label_ids = np.argmax(labels, axis=-1)
    return metric.compute(predictions=predictions, references=label_ids)

# Transformers の Trainer を用いる
# 訓練時の設定
training_args = TrainingArguments(
    output_dir="test_trainer",
    per_device_train_batch_size=8,
    num_train_epochs=1.0,
    evaluation_strategy="steps", eval_steps=200)  # 200ステップ毎にテストデータで評価

# Trainerを生成
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized_dataset,
    eval_dataset=test_tokenized_dataset,
    compute_metrics=compute_metrics,
)

# 訓練を実行
trainer.train()
trainer.save_model('/content/drive/MyDrive/Model/Trained_model')

"""# **会話インターフェース**"""

!pip install typing_extensions

# パッケージインストール
!pip install openai --quiet

# パッケージインポート
from openai import OpenAI
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML
import json

"""モデルダウンロード"""

from transformers import AutoModel, AutoTokenizer

# 保存したモデルのフォルダパス
model = AutoModel.from_pretrained('/content/drive/MyDrive/Model/Trained_model')
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# システムプロンプト
system_prompt = "You are to be treated as a friend. Please keep your sentences short and write about what you are leaning towards."

# OpenAIクライアントを初期化します。
client = OpenAI(api_key=userdata.get('openai_api'))

# 初期のシステムプロンプトと過去のメッセージを保存するリスト
past_messages = [{"role": "system", "content": system_prompt}]

# ユーザー入力ウィジェットの作成
## ユーザー入力ウィジェットの作成前にlayoutを設定します。
input_box_layout = widgets.Layout(width='100%') # 動的に変動
## ユーザー入力ウィジェットの作成時にそれを適用します。
input_box = widgets.Textarea(description='あなた:',
                             rows=10,
                             layout=input_box_layout)
send_button = widgets.Button(description='送信')

# 出力エリアの作成
output_area = widgets.Output()

#推論用関数とグラフ作成用
def np_softmax(x):
    f_x = np.exp(x) / np.sum(np.exp(x))
    return f_x

def analyze_emotion(text, show_fig=False, ret_prob=False):
    # 推論モードを有効か
    model.eval()

    # 入力データ変換 + 推論
    tokens = tokenizer(text, truncation=True, return_tensors="pt")
    tokens.to(model.device)
    preds = model(**tokens)
    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])
    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}
    out_dict_str = json.dumps(out_dict)
    
    # 棒グラフを描画
    if show_fig:
        plt.figure(figsize=(8, 3))
        df = pd.DataFrame(out_dict.items(), columns=['name', 'prob'])
        sns.barplot(x='name', y='prob', data=df)
        plt.title('入力文 : ' + text, fontsize=15)

    if ret_prob:
        return out_dict

# ユーザーからの入力を受け取ってAPIリクエストを実行する関数
def send_to_chatbot(button):
    user_input = input_box.value
    analyze_emotion(user_input, show_fig=True)
    global past_messages
    # user_input と out_dict_str を組み合わせて新しいユーザー入力メッセージを作成
    user_input_with_emotion = f"{user_input}\nEmotional Analysis Results: {out_dict_str}"

    # 新しいユーザー入力メッセージを past_messages リストに追加する
    past_messages.append({"role": "user", "content": user_input_with_emotion})

    # APIにリクエストを送信し、レスポンスを得る
    try:
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=past_messages  # 過去のメッセージを含むリストを渡す
        )
        # APIから得られたレスポンスメッセージを過去のメッセージに追加
        if completion.choices[0].message.content:
            past_messages.append({
                "role": "assistant",
                "content": completion.choices[0].message.content
            })

    except Exception as e:
        with output_area:
            clear_output()
            print(f"An error occurred: {e}")
        return

    # アウトプットエリアにメッセージを表示
    with output_area:
        clear_output()
        for message in past_messages:
            if message['role'] == 'user':
                role_label = 'You'
                color = 'blue'
            elif message['role'] == 'assistant':
                role_label = 'Assistant'
                color = 'green'
            else:  # ここで 'system' ロールを処理します
                role_label = 'System'
                color = 'gray'  # システムメッセージには異なる色を使用することもできます

            # テキスト内の改行を保持するためにwhite-spaceスタイルを使用
            display(HTML(f"<span style='color: {color}; white-space: pre-wrap;'>{role_label}: {message['content']}</span><br><br>"))


    # ユーザーの入力ボックスをクリア
    input_box.value = ''

# ボタンが押された際に上記関数を呼び出すイベントハンドラの設定
send_button.on_click(send_to_chatbot)

# ウィジェットの表示
display(output_area, widgets.HBox([input_box, send_button]))

"""## **メッセージ履歴の保存**"""

import os
import json
from datetime import datetime

# メッセージをテキストファイルに書き込む関数
def save_messages_to_text(messages, directory, filename):
    # ファイルの完全なパスを生成
    filepath = os.path.join(directory, filename)

    # 指定されたディレクトリが存在しない場合は作成
    if not os.path.exists(directory):
        os.makedirs(directory)

    with open(filepath, 'w', encoding='utf-8') as file:
        for message in messages:
            line = f"{message['role']}: {message['content']}\n\n"
            file.write(line)


    print(f"グラフは '{filepath}' に保存されました.")
    print(f"メッセージは '{filepath}' に保存されました。")

# 現在時刻の取得
current_time = datetime.now().strftime('%Y%m%d_%H%M%S')

# 保存するディレクトリとファイル名を指定
save_directory = '/content/drive/MyDrive/chat_history'
filename = f"{current_time}_chat_history.txt"

# メッセージをテキストファイルに保存
save_messages_to_text(past_messages, save_directory, filename)
